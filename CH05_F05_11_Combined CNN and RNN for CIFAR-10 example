
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, TimeDistributed, Dropout, BatchNormalization
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.regularizers import l2

# A
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# B
sequence_length = 10
x_train = x_train[:len(x_train)//sequence_length * sequence_length].reshape(-1, sequence_length, 32, 32, 3)
y_train = y_train[:len(y_train)//sequence_length * sequence_length].reshape(-1, sequence_length, 10).mean(axis=1)

x_test = x_test[:len(x_test)//sequence_length * sequence_length].reshape(-1, sequence_length, 32, 32, 3)

y_test = y_test[:len(y_test)//sequence_length * sequence_length].reshape(-1, sequence_length, 10).mean(axis=1)

# C
cnn_model = Sequential([
    Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.3),

    Conv2D(128, (3, 3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.4),

    Flatten(),
])

# D
model = Sequential([
    TimeDistributed(cnn_model, input_shape=(sequence_length, 32, 32, 3)),
    LSTM(50, dropout=0.5, recurrent_dropout=0.5),
    Dense(10, activation='softmax', kernel_regularizer=l2(0.01))
])

# E
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# F
early_stopping = EarlyStopping(monitor='val_loss', patience=10)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)
#A  Loading and Preprocessing the Data
#B Simulating Sequences
#C CNN Configuration
#D Combining CNN with RNN
#E Compiling the Model
#FCallbacks


