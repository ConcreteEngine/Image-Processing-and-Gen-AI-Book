
import os
import numpy as np
from PIL import Image
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

# Function to load images from a directory
def load_images_from_directory(dir_path):
    images = []
    for filename in os.listdir(dir_path):
        if filename.endswith(".jpg") or filename.endswith(".png"):
            img = Image.open(os.path.join(dir_path, filename))
            if img.size != (64, 64):  # Check if the image size is 64x64
                img = img.resize((64, 64))  # Resize the image if it's not
            img_data = np.array(img).flatten()  # Convert the image to a 1D array
            images.append(img_data)
    return images

# Load images from the directories
dir_path_human = '/Users/meltemballan/Documents/Image_Processing/CH5_Ballan/human'
dir_path_non_human = '/Users/meltemballan/Documents/Image_Processing/CH5_Ballan/none-human'
images_human = load_images_from_directory(dir_path_human)
images_non_human = load_images_from_directory(dir_path_non_human)

# Combine the images into one dataset
images = np.concatenate((images_human, images_non_human), axis=0)

# Normalize the image data
scaler = StandardScaler()
images_normalized = scaler.fit_transform(images)

# Define the autoencoder
input_img = Input(shape=(12288,))  # 12288 is the number of pixels in each image (64x64x3) in RGB format
encoded = Dense(128, activation='relu')(input_img) # Dense is used to add a fully connected layer to the neural network. 
encoded = Dense(64, activation='relu')(encoded) #128 and 64 are number of neurons
encoded = Dense(10, activation='relu')(encoded)  # 10 is the number of features you want

decoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(12288, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train the autoencoder
autoencoder.fit(images_normalized, images_normalized, epochs=50, batch_size=256)

# Use the encoder to reduce the dimensionality of the images
encoder = Model(input_img, encoded)
features_ae = encoder.predict(images_normalized)

# Use PCA to reduce dimensionality from 10 to 2
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(features_ae)

# Plot the result
plt.figure(figsize=(8, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1])
plt.show()

# Save the features to a text file
np.savetxt('/Users/meltemballan/Documents/Image_Processing/CH5_Ballan/AE_Results/features.txt', features_ae)

